"""APIè·¯ç”± - æä¾›å·¥å…·å’Œèµ„è®¯çš„APIæ¥å£"""
import json
import os
from pathlib import Path
from fastapi import APIRouter, Query, HTTPException
from typing import Optional
from pydantic import BaseModel
from loguru import logger
from dotenv import load_dotenv

# ç¡®ä¿åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åŠ è½½ï¼‰
try:
    env_path = Path(__file__).resolve().parent.parent.parent / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        logger.info(f"ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        logger.warning(f".env æ–‡ä»¶ä¸å­˜åœ¨: {env_path}")
except Exception as e:
    logger.warning(f"åŠ è½½ .env æ–‡ä»¶å¤±è´¥: {e}")

from ...services.data_loader import DataLoader

router = APIRouter()

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = Path(__file__).resolve().parent.parent.parent / "data" / "config.json"


class PaginatedResponse(BaseModel):
    """åˆ†é¡µå“åº”æ¨¡å‹"""
    items: list
    total: int
    page: int
    page_size: int
    total_pages: int


@router.get("/tools", response_model=PaginatedResponse)
async def get_tools(
    category: Optional[str] = Query(None, description="å·¥å…·åˆ†ç±»"),
    featured: Optional[bool] = Query(None, description="æ˜¯å¦çƒ­é—¨"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯"),
    sort_by: str = Query("score", description="æ’åºå­—æ®µï¼šscore, view_count, created_at")
):
    """è·å–å·¥å…·åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µå’Œç­›é€‰ï¼‰"""
    try:
        logger.info(f"è·å–å·¥å…·åˆ—è¡¨: category={category}, featured={featured}, page={page}, sort_by={sort_by}")
        tools, total = DataLoader.get_tools(
            category=category,
            featured=featured,
            page=page,
            page_size=page_size,
            search=search,
            sort_by=sort_by
        )
        
        logger.info(f"è·å–åˆ° {len(tools)} ä¸ªå·¥å…·ï¼Œæ€»æ•°: {total}")
        
        total_pages = (total + page_size - 1) // page_size
        
        return PaginatedResponse(
            items=tools,
            total=total,
            page=page,
            page_size=page_size,
            total_pages=total_pages
        )
    except Exception as e:
        logger.error(f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/tools/featured", response_model=PaginatedResponse)
async def get_featured_tools(
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    sort_by: str = Query("view_count", description="æ’åºå­—æ®µï¼šscore, view_count, created_at")
):
    """è·å–çƒ­é—¨å·¥å…·åˆ—è¡¨ï¼ˆæŒ‰ç‚¹å‡»é‡æ’åºï¼‰"""
    return await get_tools(
        category=None,  # çƒ­é—¨å·¥å…·ä¸é™åˆ¶åˆ†ç±»
        featured=True,
        page=page,
        page_size=page_size,
        search=None,  # çƒ­é—¨å·¥å…·ä¸æœç´¢
        sort_by=sort_by  # æŒ‰ç‚¹å‡»é‡æ’åº
    )


@router.get("/tools/{tool_id_or_identifier}")
async def get_tool_detail(tool_id_or_identifier: str):
    """
    è·å–å·¥å…·è¯¦æƒ…ï¼ˆæ”¯æŒé€šè¿‡IDæˆ–identifieræŸ¥æ‰¾ï¼‰
    
    Args:
        tool_id_or_identifier: å·¥å…·IDï¼ˆæ•°å­—ï¼‰æˆ–identifierï¼ˆå­—ç¬¦ä¸²ï¼‰
    """
    tool = None
    tool_id = None
    
    # å°è¯•æŒ‰IDæŸ¥æ‰¾ï¼ˆå¦‚æœæ˜¯æ•°å­—ï¼‰
    try:
        tool_id = int(tool_id_or_identifier)
        tool = DataLoader.get_tool_by_id(tool_id=tool_id)
    except ValueError:
        # å¦‚æœä¸æ˜¯æ•°å­—ï¼Œåˆ™æŒ‰identifieræŸ¥æ‰¾
        tool = DataLoader.get_tool_by_id(tool_identifier=tool_id_or_identifier)
    
    if not tool:
        raise HTTPException(status_code=404, detail="å·¥å…·ä¸å­˜åœ¨")
    
    # è·å–å®é™…ä½¿ç”¨çš„IDï¼ˆç”¨äºè®°å½•ç‚¹å‡»ï¼‰
    actual_tool_id = tool.get("id")
    if actual_tool_id:
        tool_id = actual_tool_id
    
    # è·å–ç›¸å…³æ–‡ç« ï¼ˆä¼˜å…ˆä½¿ç”¨ identifierï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å·¥å…·åç§°ï¼‰
    tool_name = tool.get("name", "")
    tool_identifier = tool.get("identifier")
    related_articles, total_articles = DataLoader.get_articles_by_tool(
        tool_name=tool_name,
        tool_id=tool_id,
        tool_identifier=tool_identifier,
        page=1,
        page_size=10
    )
    
    return {
        **tool,
        "related_articles": related_articles,
        "related_articles_count": total_articles
    }


@router.get("/news", response_model=PaginatedResponse)
async def get_news(
    category: Optional[str] = Query(None, description="æ–‡ç« åˆ†ç±»ï¼Œä¸ä¼ åˆ™è·å–æ‰€æœ‰æ–‡ç« ã€‚æ”¯æŒçš„å€¼ï¼šprogramming(ç¼–ç¨‹èµ„è®¯), ai_news(AIèµ„è®¯)"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯"),
    sort_by: str = Query("archived_at", description="æ’åºå­—æ®µï¼šarchived_at(å½’æ¡£æ—¶é—´ï¼Œé»˜è®¤), published_time, score(çƒ­åº¦), created_at")
):
    """
    è·å–èµ„è®¯åˆ—è¡¨ï¼ˆä¸ä¼ categoryåˆ™è·å–æ‰€æœ‰æ–‡ç« ï¼‰
    
    åˆ†ç±»æ˜ å°„å…³ç³»ï¼š
    - category="programming" -> æ–‡ä»¶: programming.json -> UIæ˜¾ç¤º: "ç¼–ç¨‹èµ„è®¯"
    - category="ai_news" -> æ–‡ä»¶: ai_news.json -> UIæ˜¾ç¤º: "AIèµ„è®¯"
    """
    try:
        articles, total = DataLoader.get_articles(
            category=category,
            page=page,
            page_size=page_size,
            search=search,
            sort_by=sort_by
        )
        
        total_pages = (total + page_size - 1) // page_size
        
        return PaginatedResponse(
            items=articles,
            total=total,
            page=page,
            page_size=page_size,
            total_pages=total_pages
        )
    except Exception as e:
        logger.error(f"è·å–èµ„è®¯åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/ai-news", response_model=PaginatedResponse)
async def get_ai_news(
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯"),
    sort_by: str = Query("archived_at", description="æ’åºå­—æ®µï¼šarchived_at(å½’æ¡£æ—¶é—´ï¼Œé»˜è®¤), published_time, score(çƒ­åº¦), created_at")
):
    """
    è·å–AIèµ„è®¯åˆ—è¡¨
    
    æ³¨æ„ï¼šæ­¤ç«¯ç‚¹å†…éƒ¨è°ƒç”¨ get_news(category="ai_news")
    - category="ai_news" -> æ–‡ä»¶: ai_news.json -> UIæ˜¾ç¤º: "AIèµ„è®¯"
    """
    return await get_news(
        category="ai_news",
        page=page,
        page_size=page_size,
        search=search,
        sort_by=sort_by
    )


@router.get("/recent", response_model=PaginatedResponse)
async def get_recent(
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯")
):
    """è·å–æœ€æ–°èµ„è®¯ï¼ˆåˆå¹¶ç¼–ç¨‹èµ„è®¯å’ŒAIèµ„è®¯ï¼ŒæŒ‰æ—¶é—´æ’åºï¼‰"""
    try:
        # è·å–æ‰€æœ‰æ–‡ç« ï¼ˆä¸åˆ†ç±»ï¼‰ï¼ŒæŒ‰å½’æ¡£æ—¶é—´æ’åº
        articles, total = DataLoader.get_articles(
            category=None,  # ä¸åˆ†ç±»ï¼Œè·å–æ‰€æœ‰æ–‡ç« 
            page=page,
            page_size=page_size,
            search=search,
            sort_by="archived_at"  # æŒ‰å½’æ¡£æ—¶é—´æ’åº
        )
        
        total_pages = (total + page_size - 1) // page_size
        
        return PaginatedResponse(
            items=articles,
            total=total,
            page=page,
            page_size=page_size,
            total_pages=total_pages
        )
    except Exception as e:
        logger.error(f"è·å–æœ€æ–°èµ„è®¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/config")
async def get_config():
    """è·å–é…ç½®æ–‡ä»¶"""
    try:
        if CONFIG_FILE.exists():
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {}


@router.post("/articles/click")
async def record_article_click_by_url(url: str = Query(..., description="æ–‡ç« URL")):
    """é€šè¿‡URLè®°å½•æ–‡ç« ç‚¹å‡»ï¼Œå¢åŠ çƒ­åº¦"""
    try:
        success = DataLoader.increment_article_view_count(url)
        if success:
            return {"ok": True, "message": "ç‚¹å‡»å·²è®°å½•"}
        else:
            raise HTTPException(status_code=500, detail="è®°å½•ç‚¹å‡»å¤±è´¥")
    except Exception as e:
        logger.error(f"è®°å½•æ–‡ç« ç‚¹å‡»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/tools/submit")
async def submit_tool(request: dict):
    """æäº¤å·¥å…·åˆ°å€™é€‰æ± """
    try:
        from ...domain.sources.tool_candidates import CandidateTool, load_candidate_pool, save_candidate_pool
        from datetime import datetime
        
        logger.info(f"æ”¶åˆ°å·¥å…·æäº¤è¯·æ±‚: {request}")
        
        name = request.get("name", "").strip()
        url = request.get("url", "").strip()
        description = request.get("description", "").strip()
        category = request.get("category", "other").strip()
        tags_str = request.get("tags", "").strip()
        icon = request.get("icon", "</>").strip()
        
        if not name or not url:
            raise HTTPException(status_code=400, detail="å·¥å…·åç§°å’Œé“¾æ¥ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯URLæ ¼å¼
        if not url.startswith(("http://", "https://")):
            raise HTTPException(status_code=400, detail="é“¾æ¥æ ¼å¼ä¸æ­£ç¡®ï¼Œå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´")
        
        # åŠ è½½ç°æœ‰å€™é€‰æ± 
        candidates = load_candidate_pool()
        logger.info(f"å½“å‰å·¥å…·å€™é€‰æ± ä¸­æœ‰ {len(candidates)} ä¸ªå·¥å…·")
        
        # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
        for candidate in candidates:
            if candidate.url == url:
                logger.warning(f"å·¥å…·å·²å­˜åœ¨äºå€™é€‰æ± : {url}")
                return {"ok": False, "message": "è¯¥å·¥å…·å·²å­˜åœ¨äºå€™é€‰æ± ä¸­"}
        
        # æ£€æŸ¥æ˜¯å¦å·²åœ¨æ­£å¼å·¥å…·æ± ä¸­
        all_tools, _ = DataLoader.get_tools(category=None, page=1, page_size=1000)
        for tool in all_tools:
            if tool.get("url") == url:
                logger.warning(f"å·¥å…·å·²å­˜åœ¨äºæ­£å¼å·¥å…·æ± : {url}")
                return {"ok": False, "message": "è¯¥å·¥å…·å·²å­˜åœ¨äºå·¥å…·åˆ—è¡¨ä¸­"}
        
        # å¤„ç†æ ‡ç­¾
        tags = []
        if tags_str:
            tags = [tag.strip() for tag in tags_str.split(",") if tag.strip()]
        
        # åˆ›å»ºå€™é€‰å·¥å…·
        new_candidate = CandidateTool(
            name=name,
            url=url,
            description=description or name,
            category=category,
            tags=tags,
            icon=icon,
            submitted_by=request.get("submitted_by", "").strip(),
            submitted_at=datetime.now().isoformat() + "Z"
        )
        
        # æ·»åŠ åˆ°å€™é€‰æ± 
        candidates.append(new_candidate)
        logger.info(f"å‡†å¤‡ä¿å­˜å€™é€‰å·¥å…·: {name}, URL: {url}, åˆ†ç±»: {category}, å€™é€‰æ± æ€»æ•°: {len(candidates)}")
        
        # ä¿å­˜å€™é€‰æ± 
        save_result = save_candidate_pool(candidates)
        logger.info(f"ä¿å­˜å·¥å…·å€™é€‰æ± ç»“æœ: {save_result}, å€™é€‰æ± å¤§å°: {len(candidates)}")
        
        if save_result:
            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜æˆåŠŸ
            from ...domain.sources.tool_candidates import _candidate_data_path
            candidate_path = _candidate_data_path()
            if candidate_path.exists():
                # é‡æ–°åŠ è½½éªŒè¯
                verify_candidates = load_candidate_pool()
                logger.info(f"éªŒè¯: é‡æ–°åŠ è½½åå·¥å…·å€™é€‰æ± å¤§å°: {len(verify_candidates)} (æœŸæœ›: {len(candidates)})")
                
                # æ£€æŸ¥æ–°æäº¤çš„å·¥å…·æ˜¯å¦åœ¨éªŒè¯åˆ—è¡¨ä¸­
                found = any(c.url == url for c in verify_candidates)
                if found:
                    logger.info("å·¥å…·å·²æˆåŠŸä¿å­˜åˆ°å€™é€‰æ± ")
                    return {"ok": True, "message": "å·¥å…·å·²æäº¤æˆåŠŸï¼Œç­‰å¾…ç®¡ç†å‘˜å®¡æ ¸"}
                else:
                    logger.error("å·¥å…·ä¿å­˜åéªŒè¯å¤±è´¥ï¼šé‡æ–°åŠ è½½çš„å€™é€‰æ± ä¸­æœªæ‰¾åˆ°æ–°æäº¤çš„å·¥å…·")
                    return {"ok": False, "message": "å·¥å…·æäº¤å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"}
            else:
                logger.error(f"å·¥å…·å€™é€‰æ± æ–‡ä»¶ä¸å­˜åœ¨: {candidate_path}")
                return {"ok": False, "message": "å·¥å…·æäº¤å¤±è´¥ï¼Œæ–‡ä»¶ä¿å­˜å¼‚å¸¸"}
        else:
            logger.error("ä¿å­˜å·¥å…·å€™é€‰æ± å¤±è´¥")
            return {"ok": False, "message": "å·¥å…·æäº¤å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤å·¥å…·å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å·¥å…·å¤±è´¥: {str(e)}")


@router.get("/admin/verify-code")
async def verify_admin_code(code: str = Query(..., description="æˆæƒç ")):
    """éªŒè¯ç®¡ç†å‘˜æˆæƒç ï¼ˆç”¨äºæ˜¾ç¤ºç®¡ç†å‘˜å…¥å£ï¼‰"""
    try:
        # è®°å½•æ¥æ”¶åˆ°çš„åŸå§‹ç¼–ç å€¼
        import urllib.parse
        decoded_code = urllib.parse.unquote(code)
        logger.info(f"æˆæƒç éªŒè¯è¯·æ±‚: åŸå§‹ç¼–ç ={code}, è§£ç å={decoded_code}, é•¿åº¦={len(decoded_code)}")
        
        # å°è¯•å¤šç§æ–¹å¼è·å–ç¯å¢ƒå˜é‡
        admin_code = os.getenv("AICODING_ADMIN_CODE", "")
        if not admin_code:
            # å°è¯•ä»ç³»ç»Ÿç¯å¢ƒå˜é‡è·å–
            admin_code = os.environ.get("AICODING_ADMIN_CODE", "")
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•é‡æ–°åŠ è½½ .env æ–‡ä»¶
        if not admin_code:
            try:
                env_path = Path(__file__).resolve().parent.parent.parent / ".env"
                if env_path.exists():
                    load_dotenv(env_path, override=True)
                    admin_code = os.getenv("AICODING_ADMIN_CODE", "")
                    logger.info(f"é‡æ–°åŠ è½½ .env æ–‡ä»¶åï¼ŒAICODING_ADMIN_CODE={'å·²è®¾ç½®' if admin_code else 'æœªè®¾ç½®'}")
            except Exception as e:
                logger.error(f"é‡æ–°åŠ è½½ .env æ–‡ä»¶å¤±è´¥: {e}")
        
        if not admin_code:
            logger.warning("AICODING_ADMIN_CODE ç¯å¢ƒå˜é‡æœªé…ç½®")
            logger.debug(f"å½“å‰æ‰€æœ‰ç¯å¢ƒå˜é‡ä¸­åŒ…å«ADMINçš„: {[k for k in os.environ.keys() if 'ADMIN' in k.upper()]}")
            logger.debug(f"å½“å‰æ‰€æœ‰ç¯å¢ƒå˜é‡: {list(os.environ.keys())[:20]}...")  # åªæ˜¾ç¤ºå‰20ä¸ª
            # æ£€æŸ¥ .env æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            env_path = Path(__file__).resolve().parent.parent.parent / ".env"
            logger.debug(f".env æ–‡ä»¶è·¯å¾„: {env_path}, å­˜åœ¨: {env_path.exists()}")
            if env_path.exists():
                try:
                    with open(env_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        logger.debug(f".env æ–‡ä»¶å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {content[:200]}")
                        if 'AICODING_ADMIN_CODE' in content:
                            logger.warning("æ£€æµ‹åˆ° .env æ–‡ä»¶ä¸­åŒ…å« AICODING_ADMIN_CODEï¼Œä½†ç¯å¢ƒå˜é‡æœªåŠ è½½ï¼Œå¯èƒ½éœ€è¦é‡å¯æœåŠ¡å™¨")
                except Exception as e:
                    logger.error(f"è¯»å– .env æ–‡ä»¶å¤±è´¥: {e}")
            return {"ok": False, "valid": False}
        
        logger.info(f"ç¯å¢ƒå˜é‡å·²åŠ è½½: é…ç½®é•¿åº¦={len(admin_code)}, é…ç½®å€¼å‰3ä¸ªå­—ç¬¦={admin_code[:3] if len(admin_code) >= 3 else admin_code}")
        
        # åŒºåˆ†å¤§å°å†™æ¯”è¾ƒ
        is_valid = decoded_code == admin_code
        logger.info(f"æˆæƒç éªŒè¯ç»“æœ: è¾“å…¥='{decoded_code}', é…ç½®='{admin_code}', åŒ¹é…={is_valid}")
        logger.info(f"å­—ç¬¦å¯¹æ¯”: è¾“å…¥é•¿åº¦={len(decoded_code)}, é…ç½®é•¿åº¦={len(admin_code)}")
        if not is_valid and len(decoded_code) == len(admin_code):
            # å¦‚æœé•¿åº¦ç›¸åŒä½†ä¸åŒ¹é…ï¼Œé€å­—ç¬¦å¯¹æ¯”
            for i, (c1, c2) in enumerate(zip(decoded_code, admin_code)):
                if c1 != c2:
                    logger.warning(f"ç¬¬{i+1}ä¸ªå­—ç¬¦ä¸åŒ¹é…: è¾“å…¥='{c1}' (ASCII {ord(c1)}), é…ç½®='{c2}' (ASCII {ord(c2)})")
        
        return {"ok": True, "valid": is_valid}
    except Exception as e:
        logger.error(f"éªŒè¯æˆæƒç æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"ok": False, "valid": False}


@router.post("/tools/{tool_id_or_identifier}/click")
async def record_tool_click(tool_id_or_identifier: str):
    """
    è®°å½•å·¥å…·ç‚¹å‡»ï¼Œå¢åŠ çƒ­åº¦ï¼ˆæ”¯æŒé€šè¿‡IDæˆ–identifieræŸ¥æ‰¾ï¼‰
    
    Args:
        tool_id_or_identifier: å·¥å…·IDï¼ˆæ•°å­—ï¼‰æˆ–identifierï¼ˆå­—ç¬¦ä¸²ï¼‰
    """
    try:
        # å°è¯•æŒ‰IDæŸ¥æ‰¾ï¼ˆå¦‚æœæ˜¯æ•°å­—ï¼‰
        tool_id = None
        tool_identifier = None
        
        try:
            tool_id = int(tool_id_or_identifier)
        except ValueError:
            # å¦‚æœä¸æ˜¯æ•°å­—ï¼Œåˆ™æŒ‰identifieræŸ¥æ‰¾
            tool_identifier = tool_id_or_identifier
        
        success = DataLoader.increment_tool_view_count(tool_id=tool_id, tool_identifier=tool_identifier)
        if success:
            return {"ok": True, "message": "ç‚¹å‡»å·²è®°å½•"}
        else:
            raise HTTPException(status_code=500, detail="è®°å½•ç‚¹å‡»å¤±è´¥")
    except Exception as e:
        logger.error(f"è®°å½•å·¥å…·ç‚¹å‡»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/articles/submit")
async def submit_article(request: dict):
    """æäº¤èµ„è®¯åˆ°å€™é€‰æ± """
    try:
        from ...domain.sources.ai_candidates import CandidateArticle, load_candidate_pool, save_candidate_pool
        from pathlib import Path
        import random
        
        logger.info(f"æ”¶åˆ°æäº¤è¯·æ±‚: {request}")
        
        title = request.get("title", "").strip()
        url = request.get("url", "").strip()
        category = request.get("category", "programming")
        summary = request.get("summary", "").strip()
        
        if not title or not url:
            raise HTTPException(status_code=400, detail="æ ‡é¢˜å’Œé“¾æ¥ä¸èƒ½ä¸ºç©º")
        
        # åŠ è½½ç°æœ‰å€™é€‰æ± 
        candidates = load_candidate_pool()
        logger.info(f"å½“å‰å€™é€‰æ± ä¸­æœ‰ {len(candidates)} ç¯‡æ–‡ç« ")
        
        # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
        for candidate in candidates:
            if candidate.url == url:
                logger.warning(f"æ–‡ç« å·²å­˜åœ¨äºå€™é€‰æ± : {url}")
                return {"ok": False, "message": "è¯¥æ–‡ç« å·²å­˜åœ¨äºå€™é€‰æ± ä¸­"}
        
        # éšæœºåˆ†é…å…³é”®å­—ï¼ˆä»é…ç½®çš„å…³é”®å­—åˆ—è¡¨ä¸­ï¼‰
        try:
            from ...config_loader import load_crawler_keywords
            keywords = load_crawler_keywords()
            if not keywords:
                # å¦‚æœæ²¡æœ‰é…ç½®å…³é”®å­—ï¼Œä½¿ç”¨é»˜è®¤å…³é”®å­—
                keywords = [
                    "AIç¼–ç¨‹", "Python", "JavaScript", "å¼€å‘å·¥å…·", "æŠ€æœ¯èµ„è®¯",
                    "ç¼–ç¨‹æŠ€å·§", "å¼€æºé¡¹ç›®", "å‰ç«¯å¼€å‘", "åç«¯å¼€å‘", "DevOps",
                    "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "Webå¼€å‘", "ç§»åŠ¨å¼€å‘", "äº‘åŸç”Ÿ"
                ]
        except Exception as e:
            logger.warning(f"åŠ è½½å…³é”®å­—é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å…³é”®å­—: {e}")
            keywords = [
                "AIç¼–ç¨‹", "Python", "JavaScript", "å¼€å‘å·¥å…·", "æŠ€æœ¯èµ„è®¯",
                "ç¼–ç¨‹æŠ€å·§", "å¼€æºé¡¹ç›®", "å‰ç«¯å¼€å‘", "åç«¯å¼€å‘", "DevOps",
                "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "Webå¼€å‘", "ç§»åŠ¨å¼€å‘", "äº‘åŸç”Ÿ"
            ]
        
        keyword = random.choice(keywords) if keywords else "ç”¨æˆ·æäº¤"
        
        # åˆ›å»ºå€™é€‰æ–‡ç« 
        source = "ç”¨æˆ·æäº¤"
        if category == "ai_news":
            source = "ç”¨æˆ·æäº¤-AIèµ„è®¯"
        elif category == "programming":
            source = "ç”¨æˆ·æäº¤-ç¼–ç¨‹èµ„è®¯"
        
        new_candidate = CandidateArticle(
            title=title,
            url=url,
            source=source,
            summary=summary or title,  # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œä½¿ç”¨æ ‡é¢˜
            crawled_from=f"user_submit:{keyword}"  # ä½¿ç”¨å…³é”®å­—ä½œä¸ºcrawled_from
        )
        
        # æ·»åŠ åˆ°å€™é€‰æ± 
        candidates.append(new_candidate)
        logger.info(f"å‡†å¤‡ä¿å­˜å€™é€‰æ–‡ç« : {title}, URL: {url}, å…³é”®å­—: {keyword}, å€™é€‰æ± æ€»æ•°: {len(candidates)}")
        
        # éªŒè¯å€™é€‰æ–‡ç« å¯¹è±¡
        try:
            from dataclasses import asdict
            candidate_dict = asdict(new_candidate)
            logger.debug(f"å€™é€‰æ–‡ç« å­—å…¸: {candidate_dict}")
        except Exception as e:
            logger.error(f"è½¬æ¢å€™é€‰æ–‡ç« ä¸ºå­—å…¸å¤±è´¥: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
        
        # ä¿å­˜å€™é€‰æ± 
        try:
            save_result = save_candidate_pool(candidates)
            logger.info(f"ä¿å­˜å€™é€‰æ± ç»“æœ: {save_result}, å€™é€‰æ± å¤§å°: {len(candidates)}")
            
            if save_result:
                # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜æˆåŠŸ
                from ...domain.sources.ai_candidates import _candidate_data_path
                candidate_path = _candidate_data_path()
                if candidate_path.exists():
                    # é‡æ–°åŠ è½½éªŒè¯
                    verify_candidates = load_candidate_pool()
                    logger.info(f"éªŒè¯: é‡æ–°åŠ è½½åå€™é€‰æ± å¤§å°: {len(verify_candidates)} (æœŸæœ›: {len(candidates)})")
                    if len(verify_candidates) >= len(candidates):
                        # æ£€æŸ¥æ–°æ–‡ç« æ˜¯å¦çœŸçš„åœ¨æ–‡ä»¶ä¸­
                        found = any(c.url == url for c in verify_candidates)
                        if found:
                            logger.info(f"ç”¨æˆ·æäº¤æ–‡ç« å·²æˆåŠŸæ·»åŠ åˆ°å€™é€‰æ± : {title} (å…³é”®å­—: {keyword})")
                            return {
                                "ok": True,
                                "message": "æäº¤æˆåŠŸï¼Œæ–‡ç« å·²è¿›å…¥å®¡æ ¸é˜Ÿåˆ—",
                                "keyword": keyword
                            }
                        else:
                            logger.error(f"éªŒè¯å¤±è´¥: æ–°æ–‡ç« æœªåœ¨é‡æ–°åŠ è½½çš„å€™é€‰æ± ä¸­æ‰¾åˆ°")
                            raise HTTPException(status_code=500, detail="ä¿å­˜éªŒè¯å¤±è´¥ï¼šæ–‡ç« æœªæ‰¾åˆ°")
                    else:
                        logger.error(f"éªŒè¯å¤±è´¥: ä¿å­˜åé‡æ–°åŠ è½½çš„å€™é€‰æ± å¤§å°ä¸åŒ¹é… (æœŸæœ›: {len(candidates)}, å®é™…: {len(verify_candidates)})")
                        raise HTTPException(status_code=500, detail="ä¿å­˜éªŒè¯å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
                else:
                    logger.error(f"ä¿å­˜åæ–‡ä»¶ä¸å­˜åœ¨: {candidate_path}")
                    raise HTTPException(status_code=500, detail="ä¿å­˜å¤±è´¥ï¼šæ–‡ä»¶æœªåˆ›å»º")
            else:
                logger.error(f"ä¿å­˜å€™é€‰æ± è¿”å›False: {title}")
                raise HTTPException(status_code=500, detail="ä¿å­˜åˆ°å€™é€‰æ± å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
        except HTTPException:
            raise
        except Exception as save_error:
            logger.error(f"ä¿å­˜å€™é€‰æ± æ—¶å‘ç”Ÿå¼‚å¸¸: {save_error}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"ä¿å­˜å¤±è´¥: {str(save_error)}")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤æ–‡ç« å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.get("/prompts", response_model=PaginatedResponse)
async def get_prompts(
    category: Optional[str] = Query(None, description="æç¤ºè¯åˆ†ç±»"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯")
):
    """è·å–æç¤ºè¯åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µå’Œç­›é€‰ï¼‰"""
    try:
        prompts, total = DataLoader.get_prompts(
            category=category,
            page=page,
            page_size=page_size,
            search=search
        )
        total_pages = (total + page_size - 1) // page_size
        return PaginatedResponse(
            items=prompts,
            total=total,
            page=page,
            page_size=page_size,
            total_pages=total_pages
        )
    except Exception as e:
        logger.error(f"è·å–æç¤ºè¯åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/prompts/{identifier}")
async def get_prompt_content(identifier: str):
    """è·å–æŒ‡å®šæç¤ºè¯çš„å†…å®¹"""
    try:
        content = DataLoader.get_prompt_content(identifier)
        if content is None:
            raise HTTPException(status_code=404, detail="æç¤ºè¯ä¸å­˜åœ¨")
        return {"content": content}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æç¤ºè¯å†…å®¹å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/rules", response_model=PaginatedResponse)
async def get_rules(
    category: Optional[str] = Query(None, description="è§„åˆ™åˆ†ç±»"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯")
):
    """è·å–è§„åˆ™åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µå’Œç­›é€‰ï¼‰"""
    try:
        rules, total = DataLoader.get_rules(
            category=category,
            page=page,
            page_size=page_size,
            search=search
        )
        total_pages = (total + page_size - 1) // page_size
        return PaginatedResponse(
            items=rules,
            total=total,
            page=page,
            page_size=page_size,
            total_pages=total_pages
        )
    except Exception as e:
        logger.error(f"è·å–è§„åˆ™åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/resources", response_model=PaginatedResponse)
async def get_resources(
    type: Optional[str] = Query(None, description="èµ„æºç±»å‹ï¼ˆæ•™ç¨‹/æ–‡ç« ï¼‰"),
    category: Optional[str] = Query(None, description="èµ„æºåˆ†ç±»"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯")
):
    """è·å–ç¤¾åŒºèµ„æºåˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µå’Œç­›é€‰ï¼‰"""
    try:
        resources, total = DataLoader.get_resources(
            type=type,
            category=category,
            page=page,
            page_size=page_size,
            search=search
        )
        total_pages = (total + page_size - 1) // page_size
        return PaginatedResponse(
            items=resources,
            total=total,
            page=page,
            page_size=page_size,
            total_pages=total_pages
        )
    except Exception as e:
        logger.error(f"è·å–ç¤¾åŒºèµ„æºåˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/weekly/{weekly_id}")
async def get_weekly(weekly_id: str):
    """è·å–æ¯å‘¨èµ„è®¯å†…å®¹"""
    try:
        # app/presentation/routes/api.py -> app/presentation/routes -> app/presentation -> app -> é¡¹ç›®æ ¹ç›®å½•
        weekly_file = Path(__file__).resolve().parent.parent.parent.parent / "data" / "weekly" / f"{weekly_id}.md"
        
        if not weekly_file.exists():
            raise HTTPException(status_code=404, detail="Weekly not found")
        
        with open(weekly_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        import re
        lines = content.split('\n')
        
        # è§£ææ–‡ç« ä¿¡æ¯
        articles = {
            'ai': [],  # AIèµ„è®¯
            'programming': []  # ç¼–ç¨‹èµ„è®¯
        }
        current_category = None
        current_article = {}
        
        title_line = ''
        time_range = ''
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # è§£æä¸»æ ‡é¢˜
            if line.startswith('# '):
                title_line = line[2:].strip()
                continue
            
            # è§£ææ—¶é—´èŒƒå›´
            if line.startswith('æ—¶é—´èŒƒå›´ï¼š'):
                time_range = line.replace('æ—¶é—´èŒƒå›´ï¼š', '').strip()
                continue
            
            # è§£æåˆ†ç±»æ ‡é¢˜
            if line.startswith('## ğŸ¤– AIèµ„è®¯'):
                current_category = 'ai'
                continue
            elif line.startswith('## ğŸ’» ç¼–ç¨‹èµ„è®¯'):
                current_category = 'programming'
                continue
            
            # è§£ææ–‡ç« æ¡ç›®ï¼ˆä»¥æ•°å­—å¼€å¤´ï¼Œå¦‚ "1. æ ‡é¢˜"ï¼‰
            if re.match(r'^\d+\.\s+', line):
                # ä¿å­˜ä¸Šä¸€ä¸ªæ–‡ç« 
                if current_article and current_category:
                    articles[current_category].append(current_article)
                
                # å¼€å§‹æ–°æ–‡ç« 
                title = re.sub(r'^\d+\.\s+', '', line).strip()
                current_article = {
                    'title': title,
                    'summary': '',
                    'source': '',
                    'url': ''
                }
                continue
            
            # è§£ææ–‡ç« è¯¦æƒ…
            if current_article:
                if line.startswith('æ¥æºï¼š'):
                    current_article['source'] = line.replace('æ¥æºï¼š', '').strip()
                elif line.startswith('é“¾æ¥ï¼š'):
                    current_article['url'] = line.replace('é“¾æ¥ï¼š', '').strip()
                elif line and not line.startswith('---') and not line.startswith('ç»Ÿè®¡ä¿¡æ¯') and not line.startswith('æœ¬æŠ¥å‘Š'):
                    # æ‘˜è¦ï¼ˆä¸æ˜¯æ¥æºã€é“¾æ¥ã€åˆ†éš”ç¬¦çš„è¡Œï¼‰
                    if not current_article['summary']:
                        current_article['summary'] = line
        
        # ä¿å­˜æœ€åä¸€ä¸ªæ–‡ç« 
        if current_article and current_category:
            articles[current_category].append(current_article)
        
        # åè½¬åˆ—è¡¨ï¼Œä½¿æœ€æ–°çš„å†…å®¹åœ¨æœ€å‰é¢
        articles['ai'].reverse()
        articles['programming'].reverse()
        
        # ç”ŸæˆHTML
        html_parts = []
        
        # æ ‡é¢˜å’Œæ—¶é—´
        html_parts.append(f'<h1 class="text-4xl tech-font-bold text-neon-cyan text-glow mb-2">{title_line}</h1>')
        if time_range:
            html_parts.append(f'<p class="text-base text-gray-400 tech-font mb-6">{time_range}</p>')
        
        # å…ˆæ˜¾ç¤ºç¼–ç¨‹èµ„è®¯
        if articles['programming']:
            html_parts.append('<h2 class="text-2xl font-bold text-gray-100 mb-4 mt-8">ğŸ’» ç¼–ç¨‹èµ„è®¯</h2>')
            html_parts.append('<div class="space-y-4 mb-8">')
            for article in articles['programming']:
                article_html = '<div class="glass rounded-lg border border-dark-border p-4 hover:border-neon-cyan transition-all">'
                if article['url']:
                    article_html += f'<a href="{article["url"]}" target="_blank" class="block">'
                    article_html += f'<h3 class="text-lg font-semibold text-neon-cyan hover:text-neon-purple mb-2 transition-colors">{article["title"]}</h3>'
                    article_html += '</a>'
                else:
                    article_html += f'<h3 class="text-lg font-semibold text-gray-100 mb-2">{article["title"]}</h3>'
                
                if article['summary'] and article['summary'] != 'æš‚æ— æ‘˜è¦':
                    article_html += f'<p class="text-sm text-gray-400 mb-2">{article["summary"]}</p>'
                
                if article['source']:
                    article_html += f'<p class="text-xs text-gray-500">æ¥æºï¼š{article["source"]}</p>'
                
                article_html += '</div>'
                html_parts.append(article_html)
            html_parts.append('</div>')
        
        # å†æ˜¾ç¤ºAIèµ„è®¯
        if articles['ai']:
            html_parts.append('<h2 class="text-2xl font-bold text-gray-100 mb-4 mt-8">ğŸ¤– AIèµ„è®¯</h2>')
            html_parts.append('<div class="space-y-4 mb-8">')
            for article in articles['ai']:
                article_html = '<div class="glass rounded-lg border border-dark-border p-4 hover:border-neon-cyan transition-all">'
                if article['url']:
                    article_html += f'<a href="{article["url"]}" target="_blank" class="block">'
                    article_html += f'<h3 class="text-lg font-semibold text-neon-cyan hover:text-neon-purple mb-2 transition-colors">{article["title"]}</h3>'
                    article_html += '</a>'
                else:
                    article_html += f'<h3 class="text-lg font-semibold text-gray-100 mb-2">{article["title"]}</h3>'
                
                if article['summary'] and article['summary'] != 'æš‚æ— æ‘˜è¦':
                    article_html += f'<p class="text-sm text-gray-400 mb-2">{article["summary"]}</p>'
                
                if article['source']:
                    article_html += f'<p class="text-xs text-gray-500">æ¥æºï¼š{article["source"]}</p>'
                
                article_html += '</div>'
                html_parts.append(article_html)
            html_parts.append('</div>')
        
        html_content = '\n'.join(html_parts)
        
        return {
            "title": title_line or weekly_id.replace('weekly', 'Week ').replace('2025', '2025 '),
            "description": time_range or "æ¯å‘¨èµ„è®¯æ±‡æ€»",
            "content": html_content
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ¯å‘¨èµ„è®¯å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/weekly")
async def list_weekly():
    """è·å–æ¯å‘¨èµ„è®¯æ–‡ä»¶åˆ—è¡¨"""
    try:
        # app/presentation/routes/api.py -> app/presentation/routes -> app/presentation -> app -> é¡¹ç›®æ ¹ç›®å½•
        weekly_dir = Path(__file__).resolve().parent.parent.parent.parent / "data" / "weekly"
        
        logger.info(f"æ¯å‘¨èµ„è®¯ç›®å½•è·¯å¾„: {weekly_dir}")
        logger.info(f"ç›®å½•æ˜¯å¦å­˜åœ¨: {weekly_dir.exists()}")
        
        if not weekly_dir.exists():
            logger.warning(f"æ¯å‘¨èµ„è®¯ç›®å½•ä¸å­˜åœ¨: {weekly_dir}")
            return {"items": []}
        
        # è·å–æ‰€æœ‰.mdæ–‡ä»¶
        weekly_files = []
        for file_path in weekly_dir.glob("*.md"):
            weekly_id = file_path.stem  # æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            # æ ¼å¼åŒ–æ˜¾ç¤ºåç§°ï¼š2025weekly49 -> 2025 Week 49
            weekly_files.append({
                "id": weekly_id,
                "name": weekly_id,
                "filename": file_path.name
            })
        
        logger.info(f"æ‰¾åˆ° {len(weekly_files)} ä¸ªæ¯å‘¨èµ„è®¯æ–‡ä»¶: {[f['id'] for f in weekly_files]}")
        
        # æŒ‰æ–‡ä»¶åå€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        weekly_files.sort(key=lambda x: x["id"], reverse=True)
        
        result = {"items": weekly_files}
        logger.info(f"è¿”å›æ•°æ®: {result}")
        return result
    except Exception as e:
        logger.error(f"è·å–æ¯å‘¨èµ„è®¯åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
